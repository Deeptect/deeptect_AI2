import argparse
import numpy as np
import torch
import torch.nn as nn
import os
from feature_extractor import extract_feature_sequence_from_video  # ðŸ‘ˆ ì¶”ë¡ ìš© feature ì¶”ì¶œ í•¨ìˆ˜ ì‚¬ìš©

# ============================
# ðŸ§  ëª¨ë¸ ì •ì˜
# ============================
class LSTMAttentionClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim=128, num_layers=2, num_classes=2):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.attn = nn.Linear(hidden_dim, 1)
        self.fc = nn.Linear(hidden_dim, num_classes)

    def forward(self, x):
        h_seq, _ = self.lstm(x)
        attn_weights = torch.softmax(self.attn(h_seq).squeeze(-1), dim=1)
        weighted_sum = torch.sum(h_seq * attn_weights.unsqueeze(-1), dim=1)
        out = self.fc(weighted_sum)
        return out

# ============================
# ðŸš€ ë‹¨ì¼ ì˜ìƒ ì¶”ë¡  ì‹¤í–‰ (ì •ë‹µë¥  ê·¹ëŒ€í™” ì „ëžµ)
# ============================
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--video_path", type=str, required=True, help="ì¶”ë¡ í•  ë™ì˜ìƒ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--model_path", type=str, default="./best_model.pth", help="í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--threshold", type=float, default=0.9, help="Originalë¡œ ê°„ì£¼í•  ìµœì†Œ í™•ë¥  (default: 0.9)")
    args = parser.parse_args()

    # ðŸ‘ï¸ Feature ì¶”ì¶œ
    print(f"ðŸ“¹ ì˜ìƒ ì²˜ë¦¬ ì¤‘: {args.video_path}")
    seq = extract_feature_sequence_from_video(args.video_path)

    if seq is None:
        print("âŒ ì–¼êµ´ ì¸ì‹ ì‹¤íŒ¨ ë˜ëŠ” ìœ íš¨ í”„ë ˆìž„ ì—†ìŒ - ì¶”ë¡  ì¤‘ë‹¨")
        exit(1)

    # ðŸ”¢ ìž…ë ¥ ì°¨ì› ë° ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    input_dim = seq.shape[1]
    model = LSTMAttentionClassifier(input_dim=input_dim)
    model.load_state_dict(torch.load(args.model_path, map_location="cpu"))

    # ðŸ§  ì¶”ë¡ 
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    x = torch.tensor(seq, dtype=torch.float32).unsqueeze(0).to(device)  # (1, 256, D)

    with torch.no_grad():
        output = model(x)
        probs = torch.softmax(output, dim=1)
        original_prob = probs[0, 0].item()
        deepfake_prob = probs[0, 1].item()

        # âœ… ì •ë‹µë¥  ê·¹ëŒ€í™”: Original í™•ë¥ ì´ ë†’ì„ ë•Œë§Œ Originalë¡œ íŒë‹¨
        if original_prob >= args.threshold:
            pred = 0  # Original
        else:
            pred = 1  # Deepfake

    print("\nâœ… ì¶”ë¡  ê²°ê³¼:")
    print(f"- Original í™•ë¥ : {original_prob:.4f}")
    print(f"- Deepfake í™•ë¥ : {deepfake_prob:.4f}")
    print(f"- ì˜ˆì¸¡ ê²°ê³¼: {'Original' if pred == 0 else 'Deepfake'} (Original ê¸°ì¤€ ìž„ê³„ê°’: {args.threshold})")
